[{"title":"Halo: a technique for visualizing off-screen objects","dblpKey":"null","doi":"10.1145/642611.642695","authors":["Patrick Baudisch","Ruth Rosenholtz"],"publisher":null,"booktitle":"ACM", "abstract":"As users pan and zoom, display content can disappear into off-screen space, particularly on small-screen devices. The clipping of locations, such as relevant places on a map, can make spatial cognition tasks harder. Halo is a visualization technique that supports spatial cognition by showing users the location of off-screen objects. Halo accomplishes this by surrounding off-screen objects with rings that are just large enough to reach into the border region of the display window. From the portion of the ring that is visible on-screen, users can infer the off-screen location of the object at the center of the ring. We report the results of a user study comparing Halo with an arrow-based visualization technique with respect to four types of map-based route planning tasks. When using the Halo interface, users completed tasks 16-33% faster, while there were no significant differences in error rate for three out of four tasks in our study."},
{"title":"Wedge: clutter-free visualization of off-screen locations","dblpKey":"null","doi":"10.1145/1357054.1357179","authors":["Sean Gustafson","Patrick Baudisch"],"publisher":null,"booktitle":"ACM", "abstract":"To overcome display limitations of small-screen devices, researchers have proposed techniques that point users to objects located off-screen. Arrow-based techniques such as City Lights convey only direction. Halo conveys direction and distance, but is susceptible to clutter resulting from overlapping halos. We present Wedge, a visualization technique that conveys direction and distance, yet avoids overlap and clutter. Wedge represents each off-screen location using an acute isosceles triangle: the tip coincides with the off-screen locations, and the two corners are located on-screen. A wedge conveys location awareness primarily by means of its two legs pointing towards the target. Wedges avoid overlap programmatically by repelling each other, causing them to rotate until overlap is resolved. As a result, wedges can be applied to numbers and configurations of targets that would lead to clutter if visualized using halos. We report on a user study comparing Wedge and Halo for three off-screen tasks. Participants were significantly more accurate when using Wedge than when using Halo."},
{"title":"Visualizing references to off-screen content on mobile devices: A comparison of Arrows, Wedge, and Overview+Detail","dblpKey":"null","doi":"10.1016/j.intcom.2011.02.005","authors":["Stefano Burigat","Luca Chittaro"],"publisher":null,"booktitle":"ACM","abstract":"When navigating large information spaces on mobile devices, the small size of the display often causes relevant content to shift off-screen, greatly increasing the difficulty of spatial tasks such as planning routes or finding points of interest on a map. Two possible approaches to mitigate the problem are Contextual Cues, i.e. visualizing abstract shapes in the border region of the view area to function as visual references to off-screen objects of interest, and Overview+Detail, i.e., simultaneously displaying a detail view and a small-scale overview of the information space. In this paper, we compare the effectiveness of two different Contextual Cues techniques, Wedge (Gustafson et al., 2008) and Scaled Arrows (Burigat et al., 2006), and a classical Overview+Detail visualization that highlights the location of objects of interest in the overview. The study involved different spatial tasks and investigated the scalability of the considered visualizations, testing them with two different numbers of off-screen objects. Results were multifaceted. With simple spatial tasks, no differences emerged among the visualizations. With more complex spatial tasks, Wedge had advantages when the task required to order off-screen objects with respect to their distance from the display window, while Overview+Detail was the best solution when users needed to find those off-screen objects that were closest to each other. Finally, we found that even a small increase in the number of off-screen objects negatively affected user performance in terms of accuracy, especially in the case of Scaled Arrows, while it had a negligible effect in terms of task completion times."},
{"title":"Visualizing locations of off-screen objects on mobile devices: a comparative evaluation of three approaches","dblpKey":"null","doi":"10.1145/1152215.1152266","authors":["Stefano Burigat","Luca Chittaro"],"publisher":null,"booktitle":"ACM","abstract":"Browsing large information spaces such as maps on the limited screen of mobile devices often requires people to perform panning and zooming operations that move relevant display content off-screen. This makes it difficult to perform spatial tasks such as finding the location of Points Of Interest (POIs) in a city. Visualizing the location of off-screen objects can mitigate this problem: in this paper, we present a user study comparing the Halo [2] approach with two other techniques based on arrows. Halo surrounds off-screen objects with circles that reach the display window, so that users can derive the location and distance of objects by observing the visible portion of the corresponding circles. In the two arrow-based techniques, arrows point at objects and their size and body length, respectively, inform about the distance of objects. Our study involved four tasks requiring users to identify and compare off-screen objects locations, and also investigated the effectiveness of the three techniques with respect to the number of off-screen objects. Arrows allowed users to order off-screen objects faster and more accurately according to their distance, while Halo allowed users to better identify the correct location of off-screen objects. Implications of these results for mobile map-based applications are also discussed."},

{"title":"On the effects of dimensionality reduction on high dimensional similarity search","dblpKey":"null","doi":"10.1145/375551.383213","authors":["Charu C. Aggarwal"],"publisher":null,"booktitle":"ACM","abstract":"The dimensionality curse has profound effects on the effectiveness of high-dimensional similarity indexing from the performance perspective. One of the well known techniques for improving the indexing performance is the method of dimensionality reduction. In this technique, the data is transformed to a lower dimensional space by finding a new axis-system in which most of the data variance is preserved in a few dimensions. This reduction may also have a positive effect on the quality of similarity for certain data domains such as text. For other domains, it may lead to loss of information and degradation of search quality. Recent research indicates that the improvement for the text domain is caused by the re-enforcement of the semantic concepts in the data. In this paper, we provide an intuitive model of the effects of dimensionality reduction on arbitrary high dimensional problems. We provide an effective diagnosis of the causality behind the qualitative effects of dimensionality reduction on a given data set. The analysis suggests that these effects are very data dependent. Our analysis also indicates that currently accepted techniques of picking the reduction which results in the least loss of information are useful for maximizing precision and recall, but are not necessarily optimum from a qualitative perspective. We demonstrate that by making simple changes to the implementation details of dimensionality reduction techniques, we can considerably improve the quality of similarity search."},
{"title":"Automatic subspace clustering of high dimensional data for data mining applications","dblpKey":"null","doi":"10.1145/276305.276314","authors":["Rakesh Agrawal","Johannes Gehrke"],"publisher":null,"booktitle":"ACM","abstract":"Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate cluster in large high dimensional datasets."},
{"title":"Can shared-neighbor distances defeat the curse of dimensionality?","dblpKey":"null","doi":"10.5555/1876037.1876078","authors":["Michael E. Houle","Hans-Peter Kriegel"],"publisher":null,"booktitle":"ACM","abstract":"The performance of similarity measures for search, indexing, and data mining applications tends to degrade rapidly as the dimensionality of the data increases. The effects of the so-called 'curse of dimensionality' have been studied by researchers for data sets generated according to a single data distribution. In this paper, we study the effects of this phenomenon on different similarity measures for multiply-distributed data. In particular, we assess the performance of shared-neighbor similarity measures, which are secondary similarity measures based on the rankings of data objects induced by some primary distance measure. We find that rank-based similarity measures can result in more stable performance than their associated primary distance measures."},
{"title":"Density-based indexing for approximate nearest-neighbor queries","dblpKey":"null","doi":"10.1145/312129.312236","authors":["Kristin P. Bennett","Usama Fayyad"],"publisher":null,"booktitle":"ACM","abstract":""},

{"title":"Staying informed: supervised and semi-supervised multi-view topical analysis of ideological perspective","dblpKey":"null","doi":"10.5555/1870658.1870769","authors":["Amr Ahmed","Eric P. Xing"],"publisher":null,"booktitle":"ACM","abstract":"With the proliferation of user-generated articles over the web, it becomes imperative to develop automated methods that are aware of the ideological-bias implicit in a document collection. While there exist methods that can classify the ideological bias of a given document, little has been done toward understanding the nature of this bias on a topical-level. In this paper we address the problem of modeling ideological perspective on a topical level using a factored topic model. We develop efficient inference algorithms using Collapsed Gibbs sampling for posterior inference, and give various evaluations and illustrations of the utility of our model on various document collections with promising results. Finally we give a Metropolis-Hasting inference algorithm for a semi-supervised extension with decent results."},
{"title":"Summarizing contrastive viewpoints in opinionated text","dblpKey":"null","doi":"10.5555/1870658.1870665","authors":["Michael J. Paul","ChengXiang Zhai"],"publisher":null,"booktitle":"ACM","abstract":"This paper presents a two-stage approach to summarizing multiple contrastive viewpoints in opinionated text. In the first stage, we use an unsupervised probabilistic approach to model and extract multiple viewpoints in text. We experiment with a variety of lexical and syntactic features, yielding significant performance gains over bag-of-words feature sets. In the second stage, we introduce Comparative LexRank, a novel random walk formulation to score sentences and pairs of sentences from opposite viewpoints based on both their representativeness of the collection as well as their contrastiveness with each other. Experimental results show that the proposed approach can generate informative summaries of viewpoints in opinionated text."},
{"title":"Which side are you on?: identifying perspectives at the document and sentence levels","dblpKey":"null","doi":"10.5555/1596276.1596297","authors":["Wei-Hao Lin","Theresa Wilson"],"publisher":null,"booktitle":"ACM","abstract":"In this paper we investigate a new problem of identifying the perspective from which a document is written. By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans. Can computers learn to identify the perspective of a document? Not every sentence is written strongly from a perspective. Can computers learn to identify which sentences strongly convey a particular perspective? We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on articles about the Israeli-Palestinian conflict. The results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy."},
{"title":"Vocabulary choice as an indicator of perspective","dblpKey":"null","doi":"10.5555/1858842.1858889","authors":["Beata Beigman Klebanov","Eyal Beigman"],"publisher":null,"booktitle":"ACM","abstract":"We establish the following characteristics of the task of perspective classification: (a) using term frequencies in a document does not improve classification achieved with absence/presence features; (b) for datasets allowing the relevant comparisons, a small number of top features is found to be as effective as the full feature set and indispensable for the best achieved performance, testifying to the existence of perspective-specific keywords. We relate our findings to research on word frequency distributions and to discourse analytic studies of perspective."},
{"title":"Modeling perspective using adaptor grammars","dblpKey":"null","doi":"10.5555/1870658.1870686","authors":["Eric A. Hardisty","Jordan Boyd-Graber"],"publisher":null,"booktitle":"ACM","abstract":"Strong indications of perspective can often come from collocations of arbitrary length; for example, someone writing get the government out of my X is typically expressing a conservative rather than progressive viewpoint. However, going beyond unigram or bigram features in perspective classification gives rise to problems of data sparsity. We address this problem using nonparametric Bayesian modeling, specifically adaptor grammars (Johnson et al., 2006). We demonstrate that an adaptive naïve Bayes model captures multiword lexical usages associated with perspective, and establishes a new state-of-the-art for perspective classification results using the Bitter Lemons corpus, a collection of essays about mid-east issues from Israeli and Palestinian points of view."},
{"title":"Mining contrastive opinions on political texts using cross-perspective topic model","dblpKey":"null","doi":"10.1145/2124295.2124306","authors":["Yi Fang","Luo Si"],"publisher":null,"booktitle":"ACM","abstract":"This paper presents a novel opinion mining research problem, which is called Contrastive Opinion Modeling (COM). Given any query topic and a set of text collections from multiple perspectives, the task of COM is to present the opinions of the individual perspectives on the topic, and furthermore to quantify their difference. This general problem subsumes many interesting applications, including opinion summarization and forecasting, government intelligence and cross-cultural studies. We propose a novel unsupervised topic model for contrastive opinion modeling. It simulates the generative process of how opinion words occur in the documents of different collections. The ad hoc opinion search process can be efficiently accomplished based on the learned parameters in the model. The difference of perspectives can be quantified in a principled way by the Jensen-Shannon divergence among the individual topic-opinion distributions. An extensive set of experiments have been conducted to evaluate the proposed model on two datasets in the political domain: 1) statement records of U.S. senators; 2) world news reports from three representative media in U.S., China and India, respectively. The experimental results with both qualitative and quantitative analysis have shown the effectiveness of the proposed model."},

{"title":"Hexastore: sextuple indexing for semantic web data management","dblpKey":"null","doi":"10.14778/1453856.1453965","authors":["Cathrin Weiss","Panagiotis Karras"],"publisher":null,"booktitle":"ACM","abstract":"Despite the intense interest towards realizing the Semantic Web vision, most existing RDF data management schemes are constrained in terms of efficiency and scalability. Still, the growing popularity of the RDF format arguably calls for an effort to offset these drawbacks. Viewed from a relational-database perspective, these constraints are derived from the very nature of the RDF data model, which is based on a triple format. Recent research has attempted to address these constraints using a vertical-partitioning approach, in which separate two-column tables are constructed for each property. However, as we show, this approach suffers from similar scalability drawbacks on queries that are not bound by RDF property value. In this paper, we propose an RDF storage scheme that uses the triple nature of RDF as an asset. This scheme enhances the vertical partitioning idea and takes it to its logical conclusion. RDF data is indexed in six possible ways, one for each possible ordering of the three RDF elements. Each instance of an RDF element is associated with two vectors; each such vector gathers elements of one of the other types, along with lists of the third-type resources attached to each vector element. Hence, a sextuple-indexing scheme emerges. This format allows for quick and scalable general-purpose query processing; it confers significant advantages (up to five orders of magnitude) compared to previous approaches for RDF data management, at the price of a worst-case five-fold increase in index space. We experimentally document the advantages of our approach on real-world and synthetic data sets with practical queries."},
{"title":"Binary RDF for scalable publishing, exchanging and consumption in the web of data","dblpKey":"null","doi":"10.1145/2187980.2187997","authors":["Javier D. Fernández"],"publisher":null,"booktitle":"ACM","abstract":"The Web of Data is increasingly producing large RDF datasets from diverse fields of knowledge, pushing the Web to a data-to-data cloud. However, traditional RDF representations were inspired by a document-centric view, which results in verbose/redundant data, costly to exchange and post-process. This article discusses an ongoing doctoral thesis addressing efficient formats for publication, exchange and consumption of RDF on a large scale. First, a binary serialization format for RDF, called HDT, is proposed. Then, we focus on compressed rich-functional structures which take part of efficient HDT representation as well as most applications performing on huge RDF datasets."},
{"title":"A complete translation from SPARQL into efficient SQL.","dblpKey":"conf/ideas/ElliottCTO09","doi":"10.1145/1620432.1620437","authors":["Brendan Elliott","En Cheng","Chimezie Thomas-Ogbuji","Z. Meral ","Özsoyoglu"],"publisher":null,"booktitle":"IDEAS", "abstract": "This paper presents a feature-complete translation from SPARQL, the proposed standard for RDF querying, into efficient SQL. We propose \"SQL model\"-based algorithms that implement each SPARQL algebra operator via SQL query augmentation, and generate a flat SQL statement for efficient processing by relational database query engines. SPARQL-to-SQL translation presented is feature-complete, since it applies to all SPARQL language features. Finally, we demonstrate the performance and scalability of our method by an extensive evaluation using recent SPARQL benchmark queries, and a benchmark dataset, as well as a real-world photo dataset."},

{"title":"Dynamic external hashing: the limit of buffering.","dblpKey":"conf/spaa/WeiYZ09","doi":"10.1145/1583991.1584055","authors":["Zhewei Wei","Ke Yi 0001","Qin Zhang 0001"],"publisher":null,"booktitle":"SPAA","abstract":"Hash tables are one of the most fundamental data structures in computer science, in both theory and practice. They are especially useful in external memory, where their query performance approaches the ideal cost of just one disk access. Knuth [16] gave an elegant analysis showing that with some simple collision resolution strategies such as linear probing or chaining, the expected average number of disk I/Os of a lookup is merely 1+1/2Ω(b), where each I/O can read and/or write a disk block containing b items. Inserting a new item into the hash table also costs 1+1/2Ω(b) I/Os, which is again almost the best one can do if the hash table is entirely stored on disk. However, this requirement is unrealistic since any algorithm operating on an external hash table must have some internal memory (at least Ω(1) blocks) to work with. The availability of a small internal memory buffer can dramatically reduce the amortized insertion cost to o(1) I/Os for many external memory data structures. In this paper we study the inherent query-insertion tradeoff of external hash tables in the presence of a memory buffer. In particular, we show that for any constant c>1, if the expected average successful query cost is targeted at 1+O(1/bc) I/Os, then it is not possible to support insertions in less than 1-O(1/bc-1/6) I/Os amortized, which means that the memory buffer is essentially useless. While if the query cost is relaxed to 1+O(1/bc) I/Os for any constant c<1, there is a simple dynamic hash table with o(1) insertion cost."},
{"title":"Linear hashing with separators—a dynamic hashing scheme achieving one-access","dblpKey":"null","doi":"10.1145/44498.44500","authors":["Per-Ake Larson"],"publisher":null,"booktitle":"ACM","abstract":"A new dynamic hashing scheme is presented. Its most outstanding feature is that any record can be retrieved in exactly one disk access. This is achieved by using a small amount of supplemental internal storage that stores enough information to uniquely determine the current location of any record. The amount of internal storage required is small: typically one byte for each page of the file. The necessary address computation, insertion, and expansion algorithms are presented and the performance is studied by means of simulation. The new method is the first practical method offering one-access retrieval for large dynamic files."},
{"title":"External-memory algorithms and data structures","dblpKey":"null","doi":"10.5555/1882757.1882767","authors":["Lars Arge","Norbert Zeh"],"publisher":null,"booktitle":"ACM","abstract":"Many modern scientific and business applications, such as, for example, geographic informationsystems, data mining, and genomic applications, store and process datasets much larger than themain memory of even state-of-the-art high-end computers. In such cases, the input/output (I/O)communication between internal and external memory (such as disks) can become a major per-formance bottleneck. Therefore, external-memory (or I/O-efficient) algorithms and data structures,which focus on minimizing the number of disk accesses used to solve a given problem, have receivedconsiderable attention in recent years."},
{"title":"External hashing with limited internal storage","dblpKey":"null","doi":"10.1145/42267.42274","authors":["Gaston H. Gonnet"],"publisher":null,"booktitle":"ACM","abstract":"The following problem is studied: How, and to what extent, can the retrieval speed of external hashing be improved by storing a small amount of extra information in internal storage? Several algorithms that guarantee retrieval in one access are developed and analyzed. In the first part of the paper, a restricted class of algorithms is studied, and a lower bound on the amount of extra storage is derived. An algorithm that achieves this bound, up to a constant difference, is also given. In the second part of the paper a number of restrictions are relaxed and several more practical algorithms are developed and analyzed. The last one, in particular, is very simple and efficient, allowing retrieval in one access using only a fixed number of bits of extra internal storage per bucket. The amount of extra internal storage depends on several factors, but it is typically very small: only a fraction of a bit per record stored. The cost of inserting a record is also analyzed and found to be low. Taking all factors into account, this algorithm is highly competitive for applications requiring very fast retrieval."},
{"title":"File organization using composite perfect hashing","dblpKey":"null","doi":"10.1145/63500.63521","authors":["M. V. Ramakrishna"],"publisher":null,"booktitle":"ACM","abstract":"Perfect hashing refers to hashing with no overflows. We propose and analyze a composite perfect hashing scheme for large external files. The scheme guarantees retrieval of any record in a single disk access. Insertions and deletions are simple, and the file size may vary considerably without adversely affecting the performance. A simple variant of the scheme supports efficient range searches in addition to being a completely dynamic file organization scheme. These advantages are achieved at the cost of a small amount of additional internal storage and increased cost of insertions."},

{"title":"Scatter/Gather: a cluster-based approach to browsing large document collections","dblpKey":"null","doi":"10.1145/133160.133214","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"Document clustering has not been well received as an information retrieval tool. Objections to its use fall into two main categories: first, that clustering is too slow for large corpora (with running time often quadratic in the number of documents); and second, that clustering does not appreciably improve retrieval. We argue that these problems arise only when clustering is used in an attempt to improve conventional search techniques. However, looking at clustering as an information access tool in its own right obviates these objections, and provides a powerful new access paradigm. We present a document browsing technique that employs document clustering as its primary operation. We also present fast (linear time) clustering algorithms which support this interactive browsing paradigm."},
{"title":"Reexamining the cluster hypothesis: scatter/gather on retrieval results","dblpKey":"null","doi":"10.1145/243199.243216","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"null"},
{"title":"A survey of Web clustering engines","dblpKey":"null","doi":"10.1145/1541880.1541884","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"Web clustering engines organize search results by topic, thus offering a complementary view to the flat-ranked list returned by conventional search engines. In this survey, we discuss the issues that must be addressed in the development of a Web clustering engine, including acquisition and preprocessing of search results, their clustering and visualization. Search results clustering, the core of the system, has specific requirements that cannot be addressed by classical clustering algorithms. We emphasize the role played by the quality of the cluster labels as opposed to optimizing only the clustering structure. We highlight the main characteristics of a number of existing Web clustering engines and also discuss how to evaluate their retrieval performance. Some directions for future research are finally presented."},
{"title":"Beyond precision@10: clustering the long tail of web search results","dblpKey":"null","doi":"10.1145/2063576.2063910","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"The paper addresses the missing user acceptance of web search result clustering. We report on selected analyses and propose new concepts to improve existing result clustering approaches. Our findings in a nutshell are: 1. Don't compete with a search engine's top hits. In response to a query we presume search engines to return an optimal result list in the sense of the probabilistic ranking principle: documents that are expected by the majority of users are placed on top and form the result list head. We argue that, with respect to the top results, it is not beneficial to replace this established form of result presentation. 2. Improve document access in the result list tail. Documents that address the information need of \"minorities\" appear at some position in the result list tail. Especially for ambiguous and multi-faceted queries we expect this tail to be long, with many users appreciating different documents. In this situation web search result clustering can improve user satisfaction by reorganizing the long tail into topic-specific clusters. 3. Avoid shadowing when constructing cluster labels. We show that most of the cluster labels that are generated by current clustering technology occur within the snippets of the result list head--an effect which we call shadowing. The value of such labels for topic organization and navigating within a clustering of the entire result list is limited. We propose and analyze a filtering approach to significantly alleviate the label shadowing effect."},
{"title":"Essential Pages.","dblpKey":"conf/webi/SwaminathanMK09","doi":"10.1109/WI-IAT.2009.33","authors":["Ashwin Swaminathan","Cherian V. Mathew","Darko Kirovski"],"publisher":null,"booktitle":"Web Intelligence", "abstract":"Results to Web search queries are ranked using heuristics that typically analyze the global link topology, user behavior, and content relevance. We point to a particular inefficiency of such methods: information redundancy. In queries where learning about a subject is an objective, modern search engines return relatively unsatisfactory results as they consider the query coverage by each page individually, not a set of pages as a whole. We address this problem using essential pages. If we denote as $mathbb{S}_Q$ the total knowledge that exists on the Web about a given query $Q$, we want to build a search engine that returns a set of essential pages $E_Q$ that maximizes the information covered over $mathbb{S}_Q$. We present a preliminary prototype that optimizes the selection of essential pages; we draw some informal comparisons with respect to existing search engines; and finally, we evaluate our prototype using a blind-test user study."},
{"title":"A hierarchical monothetic document clustering algorithm for summarization and browsing search results","dblpKey":"null","doi":"10.1145/988672.988762","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"Organizing Web search results into a hierarchy of topics and sub-topics facilitates browsing the collection and locating results of interest. In this paper, we propose a new hierarchical monothetic clustering algorithm to build a topic hierarchy for a collection of search results retrieved in response to a query. At every level of the hierarchy, the new algorithm progressively identifies topics in a way that maximizes the coverage while maintaining distinctiveness of the topics. We refer the proposed algorithm to as DisCover. Evaluating the quality of a topic hierarchy is a non-trivial task, the ultimate test being user judgment. We use several objective measures such as coverage and reach time for an empirical comparison of the proposed algorithm with two other monothetic clustering algorithms to demonstrate its superiority. Even though our algorithm is slightly more computationally intensive than one of the algorithms, it generates better hierarchies. Our user studies also show that the proposed algorithm is superior to the other algorithms as a summarizing and browsing tool."},
{"title":"Topical query decomposition","dblpKey":"null","doi":"10.1145/1401890.1401902","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"We introduce the problem of query decomposition, where we are given a query and a document retrieval system, and we want to produce a small set of queries whose union of resulting documents corresponds approximately to that of the original query. Ideally, these queries should represent coherent, conceptually well-separated topics. We provide an abstract formulation of the query decomposition problem, and we tackle it from two different perspectives. We first show how the problem can be instantiated as a specific variant of a set cover problem, for which we provide an efficient greedy algorithm. Next, we show how the same problem can be seen as a constrained clustering problem, with a very particular kind of constraint, i.e., clustering with predefined clusters. We develop a two-phase algorithm based on hierarchical agglomerative clustering followed by dynamic programming. Our experiments, conducted on a set of actual queries in a Web scale search engine, confirm the effectiveness of the proposed solutions."},

{"title":"Enhancing cluster labeling using wikipedia","dblpKey":"null","doi":"10.1145/1571941.1571967","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"This work investigates cluster labeling enhancement by utilizing Wikipedia, the free on-line encyclopedia. We describe a general framework for cluster labeling that extracts candidate labels from Wikipedia in addition to important terms that are extracted directly from the text. The \"labeling quality\" of each candidate is then evaluated by several independent judges and the top evaluated candidates are recommended for labeling. Our experimental results reveal that the Wikipedia labels agree with manual labels associated by humans to a cluster, much more than with significant terms that are extracted directly from the text. We show that in most cases even when human's associated label appears in the text, pure statistical methods have difficulty in identifying them as good descriptors. Furthermore, our experiments show that for more than 85% of the clusters in our test collection, the manual label (or an inflection, or a synonym of it) appears in the top five labels recommended by our system."},
{"title":"Topical clustering of search results.","dblpKey":"conf/wsdm/ScaiellaFMC12","doi":"10.1145/2124295.2124324","authors":["Ugo Scaiella","Paolo Ferragina","Andrea Marino","Massimiliano Ciaramita"],"publisher":null,"booktitle":"WSDM", "abstract":"Search results clustering (SRC) is a challenging algorithmic problem that requires grouping together the results returned by one or more search engines in topically coherent clusters, and labeling the clusters with meaningful phrases describing the topics of the results included in them. In this paper we propose to solve SRC via an innovative approach that consists of modeling the problem as the labeled clustering of the nodes of a newly introduced graph of topics. The topics are Wikipedia-pages identified by means of recently proposed topic annotators [9, 11, 16, 20] applied to the search results, and the edges denote the relatedness among these topics computed by taking into account the linkage of the Wikipedia-graph. We tackle this problem by designing a novel algorithm that exploits the spectral properties and the labels of that graph of topics. We show the superiority of our approach with respect to academic state-of-the-art work [6] and well-known commercial systems (CLUSTY and LINGO3G) by performing anextensive set of experiments on standard datasets and user studies via Amazon Mechanical Turk. We test several standard measures for evaluating the performance of all systems and show a relative improvement of up to 20%."},
{"title":"Frequent term-based text clustering","dblpKey":"null","doi":"10.1145/775047.775110","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"Text clustering methods can be used to structure large sets of text or hypertext documents. The well-known methods of text clustering, however, do not really address the special problems of text clustering: very high dimensionality of the data, very large size of the databases and understandability of the cluster description. In this paper, we introduce a novel approach which uses frequent item (term) sets for text clustering. Such frequent sets can be efficiently discovered using algorithms for association rule mining. To cluster based on frequent term sets, we measure the mutual overlap of frequent sets with respect to the sets of supporting documents. We present two algorithms for frequent term-based text clustering, FTC which creates flat clusterings and HFTC for hierarchical clustering. An experimental evaluation on classical text documents as well as on web documents demonstrates that the proposed algorithms obtain clusterings of comparable quality significantly more efficiently than state-of-the- art text clustering algorithms. Furthermore, our methods provide an understandable description of the discovered clusters by their frequent term sets."},
{"title":"Topical query decomposition","dblpKey":"null","doi":"10.1145/1401890.1401902","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"We introduce the problem of query decomposition, where we are given a query and a document retrieval system, and we want to produce a small set of queries whose union of resulting documents corresponds approximately to that of the original query. Ideally, these queries should represent coherent, conceptually well-separated topics.We provide an abstract formulation of the query decomposition problem, and we tackle it from two different perspectives. We first show how the problem can be instantiated as a specific variant of a set cover problem, for which we provide an efficient greedy algorithm. Next, we show how the same problem can be seen as a constrained clustering problem, with a very particular kind of constraint, i.e., clustering with predefined clusters. We develop a two-phase algorithm based on hierarchical agglomerative clustering followed by dynamic programming. Our experiments, conducted on a set of actual queries in a Web scale search engine, confirm the effectiveness of the proposed solutions."},

{"title":"A source independent framework for research paper recommendation.","dblpKey":"conf/jcdl/NascimentoLSG11","doi":"10.1145/1998076.1998132","authors":["Cristiano Nascimento","Alberto H. F. Laender","Altigran Soares da Silva","Marcos Andr","é Gon","çalves"],"publisher":null,"booktitle":"JCDL", "abstract":"As the number of research papers available on the Web has increased enormously over the years, paper recommender systems have been proposed to help researchers on automatically finding works of interest. The main problem with the current approaches is that they assume that recommending algorithms are provided with a rich set of evidence (e.g., document collections, citations, profiles) which is normally not widely available. In this paper we propose a novel source independent framework for research paper recommendation. The framework requires as input only a single research paper and generates several potential queries by using terms in that paper, which are then submitted to existing Web information sources that hold research papers. Once a set of candidate papers for recommendation is generated, the framework applies content-based recommending algorithms to rank the candidates in order to recommend the ones most related to the input paper. This is done by using only publicly available metadata (i.e., title and abstract). We evaluate our proposed framework by performing an extensive experimentation in which we analyzed several strategies for query generation and several ranking strategies for paper recommendation. Our results show that good recommendations can be obtained with simple and low cost strategies."},
{"title":"SOFIA SEARCH: a tool for automating related-work search","dblpKey":"null","doi":"10.1145/2213836.2213915","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"When working on a new project, researchers need to devote a significant amount of time and effort to surveying the relevant literature. This is required in order to gain expertise, evaluate the significance of their work and gain useful insights about a particular scientific domain. While necessary, relevant-work search is also a time-consuming and arduous process, requiring the continuous participation of the user. In this work, we introduce Sofia Search, a tool that fully automates the search and retrieval of the literature related to a topic. Given a seed of papers submitted by the user, Sofia Search searches the Web for candidate related papers, evaluates their relevance to the seed and downloads them for the user. The tool also provides modules for the evaluation and ranking of authors and papers, in the context of the retrieved papers. In the demo, we will demonstrate the functionality of our tool, by allowing users to use it via a simple and intuitive interface."},
{"title":"Recommending citations: translating papers into references","dblpKey":"null","doi":"10.1145/2396761.2398542","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"When we write or prepare to write a research paper, we always have appropriate references in mind. However, there are most likely references we have missed and should have been read and cited. As such a good citation recommendation system would not only improve our paper but, overall, the efficiency and quality of literature search. Usually, a citation's context contains explicit words explaining the citation. Using this, we propose a method that \"translates\" research papers into references. By considering the citations and their contexts from existing papers as parallel data written in two different \"languages\", we adopt the translation model to create a relationship between these two \"vocabularies\". Experiments on both CiteSeer and CiteULike dataset show that our approach outperforms other baseline methods and increase the precision, recall and f-measure by at least 5% to 10%, respectively. In addition, our approach runs much faster in the both training and recommending stage, which proves the effectiveness and the scalability of our work."},
{"title":"Context-aware citation recommendation","dblpKey":"null","doi":"10.1145/1772690.1772734","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"When you write papers, how many times do you want to make some citations at a place but you are not sure which papers to cite? Do you wish to have a recommendation system which can recommend a small number of good candidates for every place that you want to make some citations? In this paper, we present our initiative of building a context-aware citation recommendation system. High quality citation recommendation is challenging: not only should the citations recommended be relevant to the paper under composition, but also should match the local contexts of the places citations are made. Moreover, it is far from trivial to model how the topic of the whole paper and the contexts of the citation places should affect the selection and ranking of citations. To tackle the problem, we develop a context-aware approach. The core idea is to design a novel non-parametric probabilistic model which can measure the context-based relevance between a citation context and a document. Our approach can recommend citations for a context effectively. Moreover, it can recommend a set of citations for a paper with high quality. We implement a prototype system in CiteSeerX. An extensive empirical evaluation in the CiteSeerX digital library against many baselines demonstrates the effectiveness and the scalability of our approach."},

{"title":"Model-driven formative evaluation of exploratory search: A study under a sensemaking framework Share on","dblpKey":"null","doi":"10.5555/1347449.1347525","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"The evaluation of exploratory search relies on the ongoing paradigm shift from focusing on the search algorithm to focusing on the interactive process. This paper proposes a model-driven formative evaluation approach, in which the goal is not the evaluation of a specific system, per se, but the exploration of new design possibilities. This paper gives an example of this approach where a model of sensemaking was used to inform the evaluation of a basic exploratory search system(s) in the context of a sensemaking task. The model suggested that, rather than just looking at simple search performance measures, we should examine closely the interwoven, interactive processes of both representation construction and information seeking. Participants were asked to make sense of an unfamiliar topic using an augmented query-based search system. The processes of representation construction and information seeking were captured and analyzed using data from experiment notes, interviews, and a system log. The data analysis revealed users' sources of ideas for structuring representations and a tightly coupled relationship between search and representation construction in their exploratory searches. For example, users strategically used search to find useful structure ideas instead of just accumulating information facts. Implications for improving current search systems and designing new systems are discussed."},
{"title":"Exploratory search: from finding to understanding","dblpKey":"null","doi":"10.1145/1121949.1121979","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"Research tools critical for exploratory search success involve the creation of new interfaces that move the process beyond predictable fact retrieval."},
{"title":"Model-driven formative evaluation of exploratory search: A study under a sensemaking framework","dblpKey":"null","doi":"10.5555/1347449.1347525","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"The evaluation of exploratory search relies on the ongoing paradigm shift from focusing on the search algorithm to focusing on the interactive process. This paper proposes a model-driven formative evaluation approach, in which the goal is not the evaluation of a specific system, per se, but the exploration of new design possibilities. This paper gives an example of this approach where a model of sensemaking was used to inform the evaluation of a basic exploratory search system(s) in the context of a sensemaking task. The model suggested that, rather than just looking at simple search performance measures, we should examine closely the interwoven, interactive processes of both representation construction and information seeking. Participants were asked to make sense of an unfamiliar topic using an augmented query-based search system. The processes of representation construction and information seeking were captured and analyzed using data from experiment notes, interviews, and a system log. The data analysis revealed users' sources of ideas for structuring representations and a tightly coupled relationship between search and representation construction in their exploratory searches. For example, users strategically used search to find useful structure ideas instead of just accumulating information facts. Implications for improving current search systems and designing new systems are discussed."},
{"title":"Exploratory search: from finding to understanding","dblpKey":"null","doi":"10.1145/1121949.1121979","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"Research tools critical for exploratory search success involve the creation of new interfaces that move the process beyond predictable fact retrieval."},

{"title":"Towards optimum query segmentation: in doubt without","dblpKey":"null","doi":"10.1145/2396761.2398398","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"Query segmentation is the problem of identifying those keywords in a query, which together form compound concepts or phrases like \"new york times\". Such segments can help a search engine to better interpret a user's intents and to tailor the search results more appropriately. Our contributions to this problem are threefold. (1) We conduct the first large-scale study of human segmentation behavior based on more than 500000 segmentations. (2) We show that the traditionally applied segmentation accuracy measures are not appropriate for such large-scale corpora and introduce new, more robust measures. (3) We develop a new query segmentation approach with the basic idea that, in cases of doubt, it is often better to (partially) leave queries without any segmentation. This new in-doubt-without approach chooses different segmentation strategies depending on query types. A large-scale evaluation shows substantial improvement upon the state of the art in terms of segmentation accuracy. To draw a complete picture, we also evaluate the impact of segmentation strategies on retrieval performance in a TREC setting. It turns out that more accurate segmentation not necessarily yields better retrieval performance. Based on this insight, we propose an in-doubt-without variant which achieves the best retrieval performance despite leaving many queries unsegmented. But there is still room for improvement: the optimum segmentation strategy which always chooses the segmentation that maximizes retrieval performance, significantly outperforms all other tested approaches."},
{"title":"Query segmentation based on eigenspace similarity","dblpKey":"null","doi":"10.5555/1667583.1667640","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"Query segmentation is essential to query processing. It aims to tokenize query words into several semantic segments and help the search engine to improve the precision of retrieval. In this paper, we present a novel unsupervised learning approach to query segmentation based on principal eigenspace similarity of query-word-frequency matrix derived from web statistics. Experimental results show that our approach could achieve superior performance of 35.8% and 17.7% in F-measure over the two baselines respectively, i.e. MI (Mutual Information) approach and EM optimization approach."},
{"title":"Unsupervised query segmentation using click data: preliminary results","dblpKey":"null","doi":"10.1145/1772690.1772839","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"We describe preliminary results of experiments with an unsupervised framework for query segmentation, transforming keyword queries into structured queries. The resulting queries can be used to more accurately search product databases, and potentially improve result presentation and query suggestion. The key to developing an accurate and scalable system for this task is to train a query segmentation or attribute detection system over labeled data, which can be acquired automatically from query and click-through logs. The main contribution of our work is a new method to automatically acquire such training data - resulting in significantly higher segmentation performance, compared to previously reported methods."},
{"title":"An IR-based evaluation framework for web search query segmentation","dblpKey":"null","doi":"10.1145/2348283.2348401","authors":["",""],"publisher":null,"booktitle":"ACM","abstract":"This paper presents the first evaluation framework for Web search query segmentation based directly on IR performance. In the past, segmentation strategies were mainly validated against manual annotations. Our work shows that the goodness of a segmentation algorithm as judged through evaluation against a handful of human annotated segmentations hardly reflects its effectiveness in an IR-based setup. In fact, state-of the-art algorithms are shown to perform as good as, and sometimes even better than human annotations a fact masked by previous validations. The proposed framework also provides us an objective understanding of the gap between the present best and the best possible segmentation algorithm. We draw these conclusions based on an extensive evaluation of six segmentation strategies, including three most recent algorithms, vis-a-vis segmentations from three human annotators. The evaluation framework also gives insights about which segments should be necessarily detected by an algorithm for achieving the best retrieval results. The meticulously constructed dataset used in our experiments has been made public for use by the research community."},
{"title":"Query segmentation using conditional random fields.","dblpKey":"conf/sigmod/YuS09","doi":"10.1145/1557670.1557680","authors":["Xiaohui Yu 0001","Huxia Shi"],"publisher":null,"booktitle":"KEYS", "abstract":"A growing mount of available text data are being stored in relational databases, giving rise to an increasing need for the RDBMSs to support effective text retrieval. In this paper, we address the problem of keyword query segmentation, i.e., how to group nearby keywords in a query into segments. This operation can greatly benefit both the quality and the efficiency of the subsequent search operations. Compared with previous work, the proposed approach is based on Conditional Random Fields (CRF), and provides a principled statistical model that can be learned from query logs and easily adapt to user preferences. Extensive experiments on two real datasets confirm the effectiveness the efficiency of the proposed approach."}]

